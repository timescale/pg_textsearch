name: Benchmarks

permissions:
  contents: write
  deployments: write

on:
  # Nightly benchmark
  schedule:
    - cron: '0 6 * * *'  # Every day at 6 AM UTC

  # Manual trigger
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Dataset to benchmark'
        required: true
        default: 'all'
        type: choice
        options:
          - cranfield
          - msmarco
          - wikipedia
          - all
      wikipedia_size:
        description: 'Wikipedia subset size'
        required: false
        default: '100K'
        type: choice
        options:
          - '10K'
          - '100K'
          - '1M'
          - 'full'
      msmarco_size:
        description: 'MS MARCO subset size (passages)'
        required: false
        default: 'full'
        type: choice
        options:
          - '100K'
          - '500K'
          - '1M'
          - 'full'

jobs:
  # Full benchmark suite
  full-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max for full Wikipedia

    steps:
    - uses: actions/checkout@v4

    - name: Free disk space
      run: |
        echo "Initial disk space:"
        df -h /
        # Remove unnecessary packages
        sudo rm -rf /usr/share/dotnet
        sudo rm -rf /usr/local/lib/android
        sudo rm -rf /opt/ghc
        sudo rm -rf /opt/hostedtoolcache/CodeQL
        echo "After cleanup:"
        df -h /

    - name: Set up Python 3.10
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wget ca-certificates build-essential bc
        pip install wikiextractor

        # PostgreSQL
        wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | \
            sudo apt-key add -
        echo "deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main" | \
            sudo tee /etc/apt/sources.list.d/pgdg.list
        sudo apt-get update
        sudo apt-get install -y postgresql-17 postgresql-server-dev-17

    - name: Build extension (release mode)
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        make clean
        CFLAGS="-O3 -DNDEBUG" make
        sudo make install

    - name: Configure PostgreSQL for benchmarks
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        rm -rf tmp_bench
        mkdir -p tmp_bench/socket
        initdb -D tmp_bench/data --auth-local=trust --auth-host=trust

        # Tune for benchmarking
        cat >> tmp_bench/data/postgresql.conf << EOF
        unix_socket_directories = '$PWD/tmp_bench/socket'
        shared_buffers = 1GB
        effective_cache_size = 2GB
        maintenance_work_mem = 256MB
        work_mem = 128MB
        max_connections = 20
        checkpoint_completion_target = 0.9
        wal_buffers = 64MB
        random_page_cost = 1.1
        effective_io_concurrency = 200
        pg_textsearch.index_memory_limit = 512MB
        EOF

        if ! pg_ctl start -D tmp_bench/data -l tmp_bench/postgres.log -w; then
          echo "PostgreSQL failed to start. Log:"
          cat tmp_bench/postgres.log
          exit 1
        fi
        export PGHOST="$PWD/tmp_bench/socket"
        createdb bench_test
        psql -d bench_test -c "CREATE EXTENSION pg_textsearch"
        echo "PGHOST=$PWD/tmp_bench/socket" >> $GITHUB_ENV

    - name: Determine datasets to run
      id: datasets
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "dataset=${{ github.event.inputs.dataset }}" >> $GITHUB_OUTPUT
          echo "wiki_size=${{ github.event.inputs.wikipedia_size }}" >> $GITHUB_OUTPUT
          echo "msmarco_size=${{ github.event.inputs.msmarco_size }}" >> $GITHUB_OUTPUT
        else
          # Weekly schedule: run all with full MS MARCO
          echo "dataset=all" >> $GITHUB_OUTPUT
          echo "wiki_size=100K" >> $GITHUB_OUTPUT
          echo "msmarco_size=full" >> $GITHUB_OUTPUT
        fi

    - name: Download MS MARCO dataset
      if: steps.datasets.outputs.dataset == 'msmarco' || steps.datasets.outputs.dataset == 'all'
      run: |
        cd benchmarks/datasets/msmarco
        chmod +x download.sh
        ./download.sh ${{ steps.datasets.outputs.msmarco_size }}

    - name: Download Wikipedia dataset
      if: steps.datasets.outputs.dataset == 'wikipedia' || steps.datasets.outputs.dataset == 'all'
      run: |
        cd benchmarks/datasets/wikipedia
        chmod +x download.sh
        ./download.sh ${{ steps.datasets.outputs.wiki_size }}

    - name: Run Cranfield benchmark
      if: steps.datasets.outputs.dataset == 'cranfield' || steps.datasets.outputs.dataset == 'all'
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        export PGDATABASE=bench_test

        echo "=== Cranfield Benchmark ===" | tee -a benchmark_results.txt
        echo "Start: $(date)" | tee -a benchmark_results.txt

        cd benchmarks/sql/cranfield
        time psql -f 01-load.sql 2>&1 | tee -a "$GITHUB_WORKSPACE/benchmark_results.txt"
        time psql -f 02-queries.sql 2>&1 | tee -a "$GITHUB_WORKSPACE/benchmark_results.txt"

        echo "End: $(date)" | tee -a "$GITHUB_WORKSPACE/benchmark_results.txt"

    - name: Run MS MARCO benchmark
      if: steps.datasets.outputs.dataset == 'msmarco' || steps.datasets.outputs.dataset == 'all'
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        export PGDATABASE=bench_test
        export DATA_DIR="$PWD/benchmarks/datasets/msmarco/data"

        echo "=== MS MARCO Benchmark ===" | tee -a benchmark_results.txt
        echo "Start: $(date)" | tee -a benchmark_results.txt

        time psql -f benchmarks/datasets/msmarco/load.sql 2>&1 | \
            tee -a benchmark_results.txt
        time psql -f benchmarks/datasets/msmarco/queries.sql 2>&1 | \
            tee -a benchmark_results.txt

        echo "End: $(date)" | tee -a benchmark_results.txt

    - name: Run Wikipedia benchmark
      if: steps.datasets.outputs.dataset == 'wikipedia' || steps.datasets.outputs.dataset == 'all'
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        export PGDATABASE=bench_test
        export DATA_DIR="$PWD/benchmarks/datasets/wikipedia/data"

        echo "=== Wikipedia Benchmark ===" | tee -a benchmark_results.txt
        echo "Start: $(date)" | tee -a benchmark_results.txt

        time psql -f benchmarks/datasets/wikipedia/load.sql 2>&1 | \
            tee -a benchmark_results.txt
        time psql -f benchmarks/datasets/wikipedia/queries.sql 2>&1 | \
            tee -a benchmark_results.txt

        echo "End: $(date)" | tee -a benchmark_results.txt

    - name: Generate summary and metrics
      run: |
        echo "# Benchmark Summary" > benchmark_summary.md
        echo "" >> benchmark_summary.md
        echo "**Date:** $(date -u)" >> benchmark_summary.md
        echo "**Commit:** ${{ github.sha }}" >> benchmark_summary.md
        echo "**Dataset:** ${{ steps.datasets.outputs.dataset }}" >> benchmark_summary.md
        echo "" >> benchmark_summary.md

        chmod +x benchmarks/runner/extract_metrics.sh

        # Extract metrics per dataset when running "all"
        DATASET="${{ steps.datasets.outputs.dataset }}"
        if [ "$DATASET" = "all" ]; then
          # Extract each dataset separately
          ./benchmarks/runner/extract_metrics.sh benchmark_results.txt \
              cranfield_metrics.json "cranfield" "Cranfield" || true
          ./benchmarks/runner/extract_metrics.sh benchmark_results.txt \
              msmarco_metrics.json "msmarco" "MS MARCO" || true
          ./benchmarks/runner/extract_metrics.sh benchmark_results.txt \
              wikipedia_metrics.json "wikipedia" "Wikipedia" || true
          # Also create combined for summary
          ./benchmarks/runner/extract_metrics.sh benchmark_results.txt \
              benchmark_metrics.json "all" || true
        else
          ./benchmarks/runner/extract_metrics.sh benchmark_results.txt \
              benchmark_metrics.json "$DATASET" || true
        fi

        # Add metrics to summary
        echo "## Key Metrics" >> benchmark_summary.md
        for f in *_metrics.json; do
          if [ -f "$f" ]; then
            echo "### $f" >> benchmark_summary.md
            echo '```json' >> benchmark_summary.md
            cat "$f" >> benchmark_summary.md
            echo '```' >> benchmark_summary.md
          fi
        done

        # Extract timing info
        echo "" >> benchmark_summary.md
        echo "## Timing" >> benchmark_summary.md
        grep -E "^(real|user|sys|Time:)" benchmark_results.txt >> benchmark_summary.md || true

        # Extract query latencies
        echo "" >> benchmark_summary.md
        echo "## Query Latencies" >> benchmark_summary.md
        grep -E "Execution Time:" benchmark_results.txt >> benchmark_summary.md || true

        # Extract throughput
        echo "" >> benchmark_summary.md
        echo "## Throughput" >> benchmark_summary.md
        grep -E "THROUGHPUT_RESULT:" benchmark_results.txt >> benchmark_summary.md || true

        echo "" >> benchmark_summary.md
        echo "## Full Results (last 300 lines)" >> benchmark_summary.md
        echo '```' >> benchmark_summary.md
        tail -300 benchmark_results.txt >> benchmark_summary.md
        echo '```' >> benchmark_summary.md

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.run_id }}
        path: |
          benchmark_results.txt
          benchmark_summary.md
          *_metrics.json
          *_action.json
          tmp_bench/postgres.log
        retention-days: 90

    - name: Post benchmark summary to job
      if: always()
      run: |
        if [ -f benchmark_metrics.json ]; then
          echo "### Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Documents | $(jq -r '.metrics.num_documents // "N/A"' benchmark_metrics.json) |" >> $GITHUB_STEP_SUMMARY
          echo "| Index Build | $(jq -r '.metrics.index_build_time_ms // "N/A"' benchmark_metrics.json) ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Short Query | $(jq -r '.metrics.query_latencies_ms.short_query // "N/A"' benchmark_metrics.json) ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Medium Query | $(jq -r '.metrics.query_latencies_ms.medium_query // "N/A"' benchmark_metrics.json) ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Long Query | $(jq -r '.metrics.query_latencies_ms.long_query // "N/A"' benchmark_metrics.json) ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Avg Query Latency | $(jq -r '.metrics.throughput.avg_ms_per_query // "N/A"' benchmark_metrics.json) ms |" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Format metrics for benchmark tracking
      if: always()
      id: format_metrics
      run: |
        chmod +x benchmarks/runner/format_for_action.sh
        DATASET="${{ steps.datasets.outputs.dataset }}"

        if [ "$DATASET" = "all" ]; then
          # Format each dataset separately
          for name in cranfield msmarco wikipedia; do
            if [ -f "${name}_metrics.json" ]; then
              ./benchmarks/runner/format_for_action.sh \
                  "${name}_metrics.json" "${name}_action.json"
            fi
          done
          echo "is_all=true" >> $GITHUB_OUTPUT
        else
          if [ -f benchmark_metrics.json ]; then
            ./benchmarks/runner/format_for_action.sh \
                benchmark_metrics.json benchmark_action.json
            echo "dataset=$DATASET" >> $GITHUB_OUTPUT
          fi
          echo "is_all=false" >> $GITHUB_OUTPUT
        fi

    - name: Publish Cranfield benchmark
      if: always() && hashFiles('cranfield_action.json') != ''
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: 'cranfield Benchmarks'
        tool: 'customSmallerIsBetter'
        output-file-path: cranfield_action.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: benchmarks
        alert-threshold: '150%'
        comment-on-alert: true
        fail-on-alert: false

    - name: Publish MS MARCO benchmark
      if: always() && hashFiles('msmarco_action.json') != ''
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: 'msmarco Benchmarks'
        tool: 'customSmallerIsBetter'
        output-file-path: msmarco_action.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: benchmarks
        alert-threshold: '150%'
        comment-on-alert: true
        fail-on-alert: false

    - name: Publish Wikipedia benchmark
      if: always() && hashFiles('wikipedia_action.json') != ''
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: 'wikipedia Benchmarks'
        tool: 'customSmallerIsBetter'
        output-file-path: wikipedia_action.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: benchmarks
        alert-threshold: '150%'
        comment-on-alert: true
        fail-on-alert: false

    - name: Publish single dataset benchmark
      if: always() && steps.format_metrics.outputs.is_all == 'false' && hashFiles('benchmark_action.json') != ''
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: '${{ steps.format_metrics.outputs.dataset }} Benchmarks'
        tool: 'customSmallerIsBetter'
        output-file-path: benchmark_action.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: benchmarks
        alert-threshold: '150%'
        comment-on-alert: true
        fail-on-alert: false

    - name: Cleanup
      if: always()
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        pg_ctl stop -D tmp_bench/data || true
        rm -rf tmp_bench
