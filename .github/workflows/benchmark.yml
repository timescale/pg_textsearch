name: Benchmarks

permissions:
  contents: write
  deployments: write

on:
  # Weekly benchmark (only if there are new commits)
  schedule:
    - cron: '0 6 * * 0'  # Every Sunday at 6 AM UTC

  # Manual trigger
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Dataset to benchmark'
        required: true
        default: 'all'
        type: choice
        options:
          - cranfield
          - msmarco
          - wikipedia
          - all
      wikipedia_size:
        description: 'Wikipedia subset size'
        required: false
        default: '100K'
        type: choice
        options:
          - '10K'
          - '100K'
          - '1M'
          - 'full'
      msmarco_size:
        description: 'MS MARCO subset size (passages)'
        required: false
        default: 'full'
        type: choice
        options:
          - '100K'
          - '500K'
          - '1M'
          - 'full'

jobs:
  # Check for new commits (scheduled runs only)
  check-commits:
    if: github.event_name == 'schedule'
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Check for commits in last 7 days
        id: check
        run: |
          COMMITS=$(git log --oneline --since="7 days ago" | wc -l)
          if [ "$COMMITS" -gt 0 ]; then
            echo "Found $COMMITS commits in last 7 days"
            echo "should_run=true" >> $GITHUB_OUTPUT
          else
            echo "No commits in last 7 days, skipping benchmark"
            echo "should_run=false" >> $GITHUB_OUTPUT
          fi

  # Full benchmark suite
  full-benchmark:
    needs: [check-commits]
    if: |
      always() &&
      (github.event_name == 'workflow_dispatch' ||
       (github.event_name == 'schedule' && needs.check-commits.outputs.should_run == 'true'))
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max for full Wikipedia

    steps:
    - uses: actions/checkout@v4

    - name: Free disk space
      run: |
        echo "Initial disk space:"
        df -h /
        # Remove unnecessary packages
        sudo rm -rf /usr/share/dotnet
        sudo rm -rf /usr/local/lib/android
        sudo rm -rf /opt/ghc
        sudo rm -rf /opt/hostedtoolcache/CodeQL
        echo "After cleanup:"
        df -h /

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wget ca-certificates build-essential bc python3-pip
        pip3 install wikiextractor

        # PostgreSQL
        wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | \
            sudo apt-key add -
        echo "deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main" | \
            sudo tee /etc/apt/sources.list.d/pgdg.list
        sudo apt-get update
        sudo apt-get install -y postgresql-17 postgresql-server-dev-17

    - name: Build extension (release mode)
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        make clean
        CFLAGS="-O3 -DNDEBUG" make
        sudo make install

    - name: Configure PostgreSQL for benchmarks
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        rm -rf tmp_bench
        mkdir -p tmp_bench/socket
        initdb -D tmp_bench/data --auth-local=trust --auth-host=trust

        # Tune for benchmarking
        cat >> tmp_bench/data/postgresql.conf << EOF
        unix_socket_directories = '$PWD/tmp_bench/socket'
        shared_buffers = 1GB
        effective_cache_size = 2GB
        maintenance_work_mem = 256MB
        work_mem = 128MB
        max_connections = 20
        checkpoint_completion_target = 0.9
        wal_buffers = 64MB
        random_page_cost = 1.1
        effective_io_concurrency = 200
        pg_textsearch.index_memory_limit = 512MB
        EOF

        if ! pg_ctl start -D tmp_bench/data -l tmp_bench/postgres.log -w; then
          echo "PostgreSQL failed to start. Log:"
          cat tmp_bench/postgres.log
          exit 1
        fi
        export PGHOST="$PWD/tmp_bench/socket"
        createdb bench_test
        psql -d bench_test -c "CREATE EXTENSION pg_textsearch"
        echo "PGHOST=$PWD/tmp_bench/socket" >> $GITHUB_ENV

    - name: Determine datasets to run
      id: datasets
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          echo "dataset=${{ github.event.inputs.dataset }}" >> $GITHUB_OUTPUT
          echo "wiki_size=${{ github.event.inputs.wikipedia_size }}" >> $GITHUB_OUTPUT
          echo "msmarco_size=${{ github.event.inputs.msmarco_size }}" >> $GITHUB_OUTPUT
        else
          # Weekly schedule: run all with full MS MARCO
          echo "dataset=all" >> $GITHUB_OUTPUT
          echo "wiki_size=100K" >> $GITHUB_OUTPUT
          echo "msmarco_size=full" >> $GITHUB_OUTPUT
        fi

    - name: Download MS MARCO dataset
      if: steps.datasets.outputs.dataset == 'msmarco' || steps.datasets.outputs.dataset == 'all'
      run: |
        cd benchmarks/datasets/msmarco
        chmod +x download.sh
        ./download.sh ${{ steps.datasets.outputs.msmarco_size }}

    - name: Download Wikipedia dataset
      if: steps.datasets.outputs.dataset == 'wikipedia' || steps.datasets.outputs.dataset == 'all'
      run: |
        cd benchmarks/datasets/wikipedia
        chmod +x download.sh
        ./download.sh ${{ steps.datasets.outputs.wiki_size }}

    - name: Run Cranfield benchmark
      if: steps.datasets.outputs.dataset == 'cranfield' || steps.datasets.outputs.dataset == 'all'
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        export PGDATABASE=bench_test

        echo "=== Cranfield Benchmark ===" | tee -a benchmark_results.txt
        echo "Start: $(date)" | tee -a benchmark_results.txt

        cd benchmarks/sql/cranfield
        time psql -f 01-load.sql 2>&1 | tee -a "$GITHUB_WORKSPACE/benchmark_results.txt"
        time psql -f 02-queries.sql 2>&1 | tee -a "$GITHUB_WORKSPACE/benchmark_results.txt"

        echo "End: $(date)" | tee -a "$GITHUB_WORKSPACE/benchmark_results.txt"

    - name: Run MS MARCO benchmark
      if: steps.datasets.outputs.dataset == 'msmarco' || steps.datasets.outputs.dataset == 'all'
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        export PGDATABASE=bench_test
        export DATA_DIR="$PWD/benchmarks/datasets/msmarco/data"

        echo "=== MS MARCO Benchmark ===" | tee -a benchmark_results.txt
        echo "Start: $(date)" | tee -a benchmark_results.txt

        time psql -f benchmarks/datasets/msmarco/load.sql 2>&1 | \
            tee -a benchmark_results.txt
        time psql -f benchmarks/datasets/msmarco/queries.sql 2>&1 | \
            tee -a benchmark_results.txt

        echo "End: $(date)" | tee -a benchmark_results.txt

    - name: Run Wikipedia benchmark
      if: steps.datasets.outputs.dataset == 'wikipedia' || steps.datasets.outputs.dataset == 'all'
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        export PGDATABASE=bench_test
        export DATA_DIR="$PWD/benchmarks/datasets/wikipedia/data"

        echo "=== Wikipedia Benchmark ===" | tee -a benchmark_results.txt
        echo "Start: $(date)" | tee -a benchmark_results.txt

        time psql -f benchmarks/datasets/wikipedia/load.sql 2>&1 | \
            tee -a benchmark_results.txt
        time psql -f benchmarks/datasets/wikipedia/queries.sql 2>&1 | \
            tee -a benchmark_results.txt

        echo "End: $(date)" | tee -a benchmark_results.txt

    - name: Generate summary and metrics
      run: |
        echo "# Benchmark Summary" > benchmark_summary.md
        echo "" >> benchmark_summary.md
        echo "**Date:** $(date -u)" >> benchmark_summary.md
        echo "**Commit:** ${{ github.sha }}" >> benchmark_summary.md
        echo "**Dataset:** ${{ steps.datasets.outputs.dataset }}" >> benchmark_summary.md
        echo "" >> benchmark_summary.md

        # Extract structured metrics
        chmod +x benchmarks/runner/extract_metrics.sh
        ./benchmarks/runner/extract_metrics.sh benchmark_results.txt benchmark_metrics.json || true

        # Add metrics to summary
        echo "## Key Metrics" >> benchmark_summary.md
        if [ -f benchmark_metrics.json ]; then
          echo '```json' >> benchmark_summary.md
          cat benchmark_metrics.json >> benchmark_summary.md
          echo '```' >> benchmark_summary.md
        fi

        # Extract timing info
        echo "" >> benchmark_summary.md
        echo "## Timing" >> benchmark_summary.md
        grep -E "^(real|user|sys|Time:)" benchmark_results.txt >> benchmark_summary.md || true

        # Extract query latencies from EXPLAIN ANALYZE
        echo "" >> benchmark_summary.md
        echo "## Query Latencies" >> benchmark_summary.md
        grep -E "Execution Time:" benchmark_results.txt >> benchmark_summary.md || true

        # Extract throughput
        echo "" >> benchmark_summary.md
        echo "## Throughput" >> benchmark_summary.md
        grep -E "THROUGHPUT_RESULT:" benchmark_results.txt >> benchmark_summary.md || true

        echo "" >> benchmark_summary.md
        echo "## Full Results (last 300 lines)" >> benchmark_summary.md
        echo '```' >> benchmark_summary.md
        tail -300 benchmark_results.txt >> benchmark_summary.md
        echo '```' >> benchmark_summary.md

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.run_id }}
        path: |
          benchmark_results.txt
          benchmark_summary.md
          benchmark_metrics.json
          tmp_bench/postgres.log
        retention-days: 90

    - name: Post benchmark summary to job
      if: always()
      run: |
        if [ -f benchmark_metrics.json ]; then
          echo "### Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Documents | $(jq -r '.metrics.num_documents // "N/A"' benchmark_metrics.json) |" >> $GITHUB_STEP_SUMMARY
          echo "| Index Build | $(jq -r '.metrics.index_build_time_ms // "N/A"' benchmark_metrics.json) ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Short Query | $(jq -r '.metrics.query_latencies_ms.short_query // "N/A"' benchmark_metrics.json) ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Medium Query | $(jq -r '.metrics.query_latencies_ms.medium_query // "N/A"' benchmark_metrics.json) ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Long Query | $(jq -r '.metrics.query_latencies_ms.long_query // "N/A"' benchmark_metrics.json) ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Avg Query Latency | $(jq -r '.metrics.throughput.avg_ms_per_query // "N/A"' benchmark_metrics.json) ms |" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Format metrics for benchmark tracking
      if: always()
      run: |
        if [ -f benchmark_metrics.json ]; then
          chmod +x benchmarks/runner/format_for_action.sh
          ./benchmarks/runner/format_for_action.sh benchmark_metrics.json benchmark_action.json
        fi

    - name: Store and publish benchmark results
      if: always() && hashFiles('benchmark_action.json') != ''
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: 'MS MARCO Benchmarks'
        tool: 'customSmallerIsBetter'
        output-file-path: benchmark_action.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: benchmarks
        alert-threshold: '150%'
        comment-on-alert: true
        fail-on-alert: false

    - name: Cleanup
      if: always()
      run: |
        export PATH="/usr/lib/postgresql/17/bin:$PATH"
        pg_ctl stop -D tmp_bench/data || true
        rm -rf tmp_bench
