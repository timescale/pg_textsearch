-- Test case: scoring1
-- Generated BM25 test with 2 documents and 2 queries
-- Testing both bulk build and incremental build modes
CREATE EXTENSION IF NOT EXISTS pg_textsearch;
INFO:  pg_textsearch v0.0.4-dev: This is prerelease software and should not be used in production.
\o /dev/null
\i test/sql/validation.sql
-- BM25 Score Validation Functions
--
-- Formula: BM25(q,d) = Î£ IDF(qi) * (tf(qi,d) * (k1 + 1)) / (tf(qi,d) + k1 * (1 - b + b * |d| / avgdl))
-- Where:
--   IDF(qi) = ln(1 + (N - df(qi) + 0.5) / (df(qi) + 0.5))
--   tf(qi,d) = term frequency of qi in document d
--   |d| = document length, avgdl = average document length
--   k1 = term frequency saturation, b = length normalization
--
-- Usage Example:
--   SELECT validate_bm25_scoring('mytable', 'content_column', 'my_bm25_idx', 'search query', 'english', 1.2, 0.75)
CREATE OR REPLACE FUNCTION validate_bm25_scoring(
    p_table_name text,
    p_text_column text,
    p_index_name text,
    p_query text,
    p_text_config text,
    p_k1 float,
    p_b float
) RETURNS boolean AS $$
DECLARE
    v_total_docs integer;
    v_avg_doc_length float8;
    v_all_match boolean;
    v_sql text;
BEGIN
    -- Step 1: Tokenize documents with proper term frequency counting
    EXECUTE format($sql$
        CREATE TEMP TABLE temp_docs ON COMMIT DROP AS
        WITH numbered AS (
            SELECT
                ROW_NUMBER() OVER () as doc_id,
                %I as content
            FROM %I
            WHERE %I IS NOT NULL
        ),
        doc_terms AS (
            SELECT
                doc_id,
                content,
                lexeme,
                array_length(string_to_array(positions::text, ','), 1) as term_count
            FROM numbered,
                 LATERAL unnest(to_tsvector('%s', content)) as t(lexeme, positions, weights)
        )
        SELECT
            doc_id,
            content,
            array_agg(lexeme ORDER BY lexeme) as unique_tokens,
            SUM(term_count) as doc_length
        FROM doc_terms
        GROUP BY doc_id, content
    $sql$, p_text_column, p_table_name, p_text_column, p_text_config);

    -- Step 2: Calculate corpus statistics
    SELECT
        COUNT(*),
        AVG(td.doc_length)::float8
    INTO v_total_docs, v_avg_doc_length
    FROM temp_docs td;

    -- Step 3: Build term frequency table with actual counts from positions
    EXECUTE format($sql$
        CREATE TEMP TABLE temp_tf ON COMMIT DROP AS
        WITH numbered AS (
            SELECT ROW_NUMBER() OVER () as doc_id, %I as content
            FROM %I WHERE %I IS NOT NULL
        )
        SELECT
            n.doc_id,
            t.lexeme as term,
            array_length(string_to_array(t.positions::text, ','), 1) as term_freq,
            (SELECT doc_length FROM temp_docs td WHERE td.doc_id = n.doc_id) as doc_length
        FROM numbered n,
             LATERAL unnest(to_tsvector('%s', n.content)) as t(lexeme, positions, weights)
    $sql$, p_text_column, p_table_name, p_text_column, p_text_config);

    -- Step 4: Build document frequency table
    CREATE TEMP TABLE temp_df ON COMMIT DROP AS
    SELECT
        term,
        COUNT(DISTINCT temp_tf.doc_id) as doc_freq
    FROM temp_tf
    GROUP BY term;

    -- Step 5: Tokenize query
    EXECUTE format($sql$
        CREATE TEMP TABLE temp_query ON COMMIT DROP AS
        SELECT DISTINCT lexeme as term
        FROM unnest(to_tsvector('%s', %L))
    $sql$, p_text_config, p_query);

    -- Step 6: Get Tapir scores
    v_sql := format($sql$
        CREATE TEMP TABLE temp_tapir_scores ON COMMIT DROP AS
        SELECT
            ROW_NUMBER() OVER () as doc_id,
            (%I <@> to_bm25query(%L, %L))::float8 as score
        FROM %I
        WHERE %I IS NOT NULL
        ORDER BY ROW_NUMBER() OVER ()
    $sql$, p_text_column, p_query, p_index_name, p_table_name, p_text_column);
    EXECUTE v_sql;

    -- Step 7: Compute BM25 scores and compare to 4 decimal places
    WITH sql_scores AS (
        SELECT
            d.doc_id,
            d.content,
            COALESCE(SUM(
                -- IDF: ln(1 + (N - df + 0.5) / (df + 0.5))
                ln(1 + (v_total_docs - df.doc_freq + 0.5)::float8 / (df.doc_freq + 0.5)) *
                -- TF component: (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * |D|/avgdl))
                (tf.term_freq * (p_k1 + 1)) /
                (tf.term_freq + p_k1 * (1 - p_b + p_b * tf.doc_length / v_avg_doc_length))
            ), 0) as bm25_score
        FROM temp_docs d
        CROSS JOIN temp_query q
        LEFT JOIN temp_tf tf ON tf.doc_id = d.doc_id AND tf.term = q.term
        LEFT JOIN temp_df df ON df.term = q.term
        GROUP BY d.doc_id, d.content
    )
    SELECT bool_and(
        -- Compare to 4 decimal places
        ROUND(s.bm25_score::numeric, 4) = ROUND((-t.score)::numeric, 4)
    ) INTO v_all_match
    FROM sql_scores s
    JOIN temp_tapir_scores t ON s.doc_id = t.doc_id;

    -- Clean up
    DROP TABLE IF EXISTS temp_docs;
    DROP TABLE IF EXISTS temp_tf;
    DROP TABLE IF EXISTS temp_df;
    DROP TABLE IF EXISTS temp_query;
    DROP TABLE IF EXISTS temp_tapir_scores;

    RETURN COALESCE(v_all_match, false);
END;
$$ LANGUAGE plpgsql;
-- Debug function to show detailed score computation
CREATE OR REPLACE FUNCTION debug_bm25_computation(
    p_table_name text,
    p_text_column text,
    p_index_name text,
    p_query text,
    p_text_config text,
    p_k1 float,
    p_b float
) RETURNS TABLE(
    doc_id integer,
    content text,
    doc_length integer,
    sql_bm25 float8,
    tapir_bm25 float8,
    difference float8,
    matches_4dp boolean,
    term_details jsonb
) AS $$
DECLARE
    v_total_docs integer;
    v_avg_doc_length float8;
    v_sql text;
BEGIN
    -- Tokenize documents with proper position counting
    EXECUTE format($sql$
        CREATE TEMP TABLE debug_docs ON COMMIT DROP AS
        WITH numbered AS (
            SELECT ROW_NUMBER() OVER () as doc_id, %I as content
            FROM %I WHERE %I IS NOT NULL
        ),
        doc_terms AS (
            SELECT
                doc_id, content, lexeme,
                array_length(string_to_array(positions::text, ','), 1) as term_count
            FROM numbered,
                 LATERAL unnest(to_tsvector('%s', content)) as t(lexeme, positions, weights)
        )
        SELECT doc_id, content,
               array_agg(DISTINCT lexeme) as unique_tokens,
               SUM(term_count) as doc_length
        FROM doc_terms
        GROUP BY doc_id, content
    $sql$, p_text_column, p_table_name, p_text_column, p_text_config);

    -- Get statistics
    SELECT COUNT(*), AVG(dd.doc_length)::float8
    INTO v_total_docs, v_avg_doc_length
    FROM debug_docs dd;

    -- Compute term frequencies with actual position counts
    EXECUTE format($sql$
        CREATE TEMP TABLE debug_tf ON COMMIT DROP AS
        WITH numbered AS (
            SELECT ROW_NUMBER() OVER () as doc_id, %I as content
            FROM %I WHERE %I IS NOT NULL
        )
        SELECT
            n.doc_id,
            t.lexeme as term,
            array_length(string_to_array(t.positions::text, ','), 1) as tf,
            (SELECT doc_length::int FROM debug_docs d WHERE d.doc_id = n.doc_id) as doc_len
        FROM numbered n,
             LATERAL unnest(to_tsvector('%s', n.content)) as t(lexeme, positions, weights)
    $sql$, p_text_column, p_table_name, p_text_column, p_text_config);

    CREATE TEMP TABLE debug_df ON COMMIT DROP AS
    SELECT term, COUNT(DISTINCT debug_tf.doc_id) as df
    FROM debug_tf GROUP BY term;

    -- Query terms
    EXECUTE format($sql$
        CREATE TEMP TABLE debug_query ON COMMIT DROP AS
        SELECT DISTINCT lexeme as term
        FROM unnest(to_tsvector('%s', %L))
    $sql$, p_text_config, p_query);

    -- Get Tapir scores
    v_sql := format($sql$
        CREATE TEMP TABLE debug_tapir_scores ON COMMIT DROP AS
        SELECT ROW_NUMBER() OVER () as doc_id,
               (%I <@> to_bm25query(%L, %L))::float8 as score
        FROM %I WHERE %I IS NOT NULL
        ORDER BY ROW_NUMBER() OVER ()
    $sql$, p_text_column, p_query, p_index_name, p_table_name, p_text_column);
    EXECUTE v_sql;

    -- Compute and return scores with details
    RETURN QUERY
    WITH score_details AS (
        SELECT
            d.doc_id,
            d.content::text,
            d.doc_length::int as doc_length,
            COALESCE(SUM(
                ln(1 + (v_total_docs - df.df + 0.5)::float8 / (df.df + 0.5)) *
                (tf.tf * (p_k1 + 1)) / (tf.tf + p_k1 * (1 - p_b + p_b * tf.doc_len / v_avg_doc_length))
            ), 0) as sql_score,
            jsonb_agg(
                CASE WHEN tf.tf IS NOT NULL THEN
                jsonb_build_object(
                    'term', q.term,
                    'tf', tf.tf,
                    'df', df.df,
                    'idf', ln(1 + (v_total_docs - df.df + 0.5)::float8 / (df.df + 0.5)),
                    'score', ln(1 + (v_total_docs - df.df + 0.5)::float8 / (df.df + 0.5)) *
                             (tf.tf * (p_k1 + 1)) / (tf.tf + p_k1 * (1 - p_b + p_b * tf.doc_len / v_avg_doc_length))
                )
                ELSE NULL END
            ) as term_details
        FROM debug_docs d
        CROSS JOIN debug_query q
        LEFT JOIN debug_tf tf ON tf.doc_id = d.doc_id AND tf.term = q.term
        LEFT JOIN debug_df df ON df.term = q.term
        GROUP BY d.doc_id, d.content, d.doc_length
    )
    SELECT
        s.doc_id::integer,
        s.content,
        s.doc_length::integer,
        s.sql_score,
        -t.score as tapir_score,
        ABS(s.sql_score - (-t.score)) as difference,
        ROUND(s.sql_score::numeric, 4) = ROUND((-t.score)::numeric, 4) as matches_4dp,
        s.term_details
    FROM score_details s
    JOIN debug_tapir_scores t ON s.doc_id = t.doc_id
    ORDER BY s.sql_score DESC;

    -- Clean up
    DROP TABLE IF EXISTS debug_docs;
    DROP TABLE IF EXISTS debug_tf;
    DROP TABLE IF EXISTS debug_df;
    DROP TABLE IF EXISTS debug_query;
    DROP TABLE IF EXISTS debug_tapir_scores;
END;
$$ LANGUAGE plpgsql;
-- Simpler comparison function
CREATE OR REPLACE FUNCTION compare_bm25_scores(
    p_table_name text,
    p_text_column text,
    p_index_name text,
    p_query text,
    p_text_config text,
    p_k1 float,
    p_b float
) RETURNS TABLE(
    doc_id integer,
    content text,
    sql_score float8,
    tapir_score float8,
    difference float8,
    matches_4dp boolean
) AS $$
DECLARE
    v_total_docs integer;
    v_avg_doc_length float8;
    v_sql text;
BEGIN
    -- Tokenize documents with position counting
    EXECUTE format($sql$
        CREATE TEMP TABLE comp_docs ON COMMIT DROP AS
        WITH numbered AS (
            SELECT ROW_NUMBER() OVER () as doc_id, %I as content
            FROM %I WHERE %I IS NOT NULL
        ),
        doc_terms AS (
            SELECT doc_id, content, lexeme,
                   array_length(string_to_array(positions::text, ','), 1) as term_count
            FROM numbered,
                 LATERAL unnest(to_tsvector('%s', content)) as t(lexeme, positions, weights)
        )
        SELECT doc_id, content,
               SUM(term_count) as doc_length
        FROM doc_terms
        GROUP BY doc_id, content
    $sql$, p_text_column, p_table_name, p_text_column, p_text_config);

    -- Get statistics
    SELECT COUNT(*), AVG(c.doc_length)::float8
    INTO v_total_docs, v_avg_doc_length
    FROM comp_docs c;

    -- Build term frequencies with position counts
    EXECUTE format($sql$
        CREATE TEMP TABLE comp_tf ON COMMIT DROP AS
        WITH numbered AS (
            SELECT ROW_NUMBER() OVER () as doc_id, %I as content
            FROM %I WHERE %I IS NOT NULL
        )
        SELECT
            n.doc_id,
            t.lexeme as term,
            array_length(string_to_array(t.positions::text, ','), 1) as tf,
            (SELECT doc_length::int FROM comp_docs c WHERE c.doc_id = n.doc_id) as doc_len
        FROM numbered n,
             LATERAL unnest(to_tsvector('%s', n.content)) as t(lexeme, positions, weights)
    $sql$, p_text_column, p_table_name, p_text_column, p_text_config);

    -- Document frequencies
    CREATE TEMP TABLE comp_df ON COMMIT DROP AS
    SELECT term, COUNT(DISTINCT comp_tf.doc_id) as df
    FROM comp_tf GROUP BY term;

    -- Query terms
    EXECUTE format($sql$
        CREATE TEMP TABLE comp_query ON COMMIT DROP AS
        SELECT DISTINCT lexeme as term
        FROM unnest(to_tsvector('%s', %L))
    $sql$, p_text_config, p_query);

    -- Tapir scores
    v_sql := format($sql$
        CREATE TEMP TABLE comp_tapir ON COMMIT DROP AS
        SELECT ROW_NUMBER() OVER () as doc_id,
               (%I <@> to_bm25query(%L, %L))::float8 as score
        FROM %I WHERE %I IS NOT NULL
        ORDER BY ROW_NUMBER() OVER ()
    $sql$, p_text_column, p_query, p_index_name, p_table_name, p_text_column);
    EXECUTE v_sql;

    -- Compute and compare
    RETURN QUERY
    WITH sql_scores AS (
        SELECT
            d.doc_id,
            d.content::text,
            COALESCE(SUM(
                ln(1 + (v_total_docs - df.df + 0.5)::float8 / (df.df + 0.5)) *
                (tf.tf * (p_k1 + 1)) / (tf.tf + p_k1 * (1 - p_b + p_b * tf.doc_len / v_avg_doc_length))
            ), 0) as bm25_score
        FROM comp_docs d
        CROSS JOIN comp_query q
        LEFT JOIN comp_tf tf ON tf.doc_id = d.doc_id AND tf.term = q.term
        LEFT JOIN comp_df df ON df.term = q.term
        GROUP BY d.doc_id, d.content
    )
    SELECT
        s.doc_id::integer,
        s.content,
        s.bm25_score as sql_score,
        -t.score as tapir_score,
        ABS(s.bm25_score - (-t.score)) as difference,
        ROUND(s.bm25_score::numeric, 4) = ROUND((-t.score)::numeric, 4) as matches_4dp
    FROM sql_scores s
    JOIN comp_tapir t ON s.doc_id = t.doc_id
    ORDER BY s.bm25_score DESC;

    -- Clean up
    DROP TABLE IF EXISTS comp_docs;
    DROP TABLE IF EXISTS comp_tf;
    DROP TABLE IF EXISTS comp_df;
    DROP TABLE IF EXISTS comp_query;
    DROP TABLE IF EXISTS comp_tapir;
END;
$$ LANGUAGE plpgsql;
\o
SET pg_textsearch.log_scores = true;
SET enable_seqscan = off;
-- MODE 1: Bulk build (insert data, then create index)
CREATE TABLE scoring1_bulk (
    id SERIAL PRIMARY KEY,
    content TEXT
);
-- Insert test documents
INSERT INTO scoring1_bulk (content) VALUES ('hello world');
INSERT INTO scoring1_bulk (content) VALUES ('goodbye cruel world');
-- Create index after data insertion (bulk build)
CREATE INDEX scoring1_bulk_idx ON scoring1_bulk USING bm25(content)
  WITH (text_config='english', k1=1.2, b=0.75);
NOTICE:  BM25 index build started for relation scoring1_bulk_idx
NOTICE:  Using text search configuration: english
NOTICE:  Using index options: k1=1.20, b=0.75
NOTICE:  BM25 index build completed: 2 documents, avg_length=2.50, text_config='english' (k1=1.20, b=0.75)
-- Bulk mode query 1: 'hello'
SELECT id, content, ROUND((content <@> to_bm25query('hello', 'scoring1_bulk_idx'))::numeric, 4) as score
FROM scoring1_bulk
ORDER BY content <@> to_bm25query('hello', 'scoring1_bulk_idx'), id;
NOTICE:  BM25 index scan: tid=(0,1), BM25_score=-0.7549
NOTICE:  BM25 index scan: tid=(0,2), BM25_score=0.0000
 id |       content       |  score  
----+---------------------+---------
  1 | hello world         | -0.7549
  2 | goodbye cruel world |  0.0000
(2 rows)

-- Validate BM25 scoring for 'hello'
SELECT validate_bm25_scoring('scoring1_bulk', 'content', 'scoring1_bulk_idx', 'hello', 'english', 1.2, 0.75) as hello_valid;
 hello_valid 
-------------
 t
(1 row)

-- Bulk mode query 2: 'cruel'
SELECT id, content, ROUND((content <@> to_bm25query('cruel', 'scoring1_bulk_idx'))::numeric, 4) as score
FROM scoring1_bulk
ORDER BY content <@> to_bm25query('cruel', 'scoring1_bulk_idx'), id;
NOTICE:  BM25 index scan: tid=(0,2), BM25_score=-0.6407
NOTICE:  BM25 index scan: tid=(0,1), BM25_score=0.0000
 id |       content       |  score  
----+---------------------+---------
  2 | goodbye cruel world | -0.6407
  1 | hello world         |  0.0000
(2 rows)

-- Validate BM25 scoring for 'cruel'
SELECT validate_bm25_scoring('scoring1_bulk', 'content', 'scoring1_bulk_idx', 'cruel', 'english', 1.2, 0.75) as cruel_valid;
 cruel_valid 
-------------
 t
(1 row)

-- MODE 2: Incremental build (create index, then insert data)
CREATE TABLE scoring1_incr (
    id SERIAL PRIMARY KEY,
    content TEXT
);
-- Create index before data insertion (incremental build)
CREATE INDEX scoring1_incr_idx ON scoring1_incr USING bm25(content)
  WITH (text_config='english', k1=1.2, b=0.75);
NOTICE:  BM25 index build started for relation scoring1_incr_idx
NOTICE:  Using text search configuration: english
NOTICE:  Using index options: k1=1.20, b=0.75
NOTICE:  BM25 index build completed: 0 documents, avg_length=0.00, text_config='english' (k1=1.20, b=0.75)
-- Insert test documents incrementally
INSERT INTO scoring1_incr (content) VALUES ('hello world');
INSERT INTO scoring1_incr (content) VALUES ('goodbye cruel world');
-- Incremental mode query 1: 'hello'
SELECT id, content, ROUND((content <@> to_bm25query('hello', 'scoring1_incr_idx'))::numeric, 4) as score
FROM scoring1_incr
ORDER BY content <@> to_bm25query('hello', 'scoring1_incr_idx'), id;
NOTICE:  BM25 index scan: tid=(0,1), BM25_score=-0.7549
NOTICE:  BM25 index scan: tid=(0,2), BM25_score=0.0000
 id |       content       |  score  
----+---------------------+---------
  1 | hello world         | -0.7549
  2 | goodbye cruel world |  0.0000
(2 rows)

-- Validate BM25 scoring for 'hello' (incremental)
SELECT validate_bm25_scoring('scoring1_incr', 'content', 'scoring1_incr_idx', 'hello', 'english', 1.2, 0.75) as hello_incr_valid;
 hello_incr_valid 
------------------
 t
(1 row)

-- Incremental mode query 2: 'cruel'
SELECT id, content, ROUND((content <@> to_bm25query('cruel', 'scoring1_incr_idx'))::numeric, 4) as score
FROM scoring1_incr
ORDER BY content <@> to_bm25query('cruel', 'scoring1_incr_idx'), id;
NOTICE:  BM25 index scan: tid=(0,2), BM25_score=-0.6407
NOTICE:  BM25 index scan: tid=(0,1), BM25_score=0.0000
 id |       content       |  score  
----+---------------------+---------
  2 | goodbye cruel world | -0.6407
  1 | hello world         |  0.0000
(2 rows)

-- Validate BM25 scoring for 'cruel' (incremental)
SELECT validate_bm25_scoring('scoring1_incr', 'content', 'scoring1_incr_idx', 'cruel', 'english', 1.2, 0.75) as cruel_incr_valid;
 cruel_incr_valid 
------------------
 t
(1 row)

-- Cleanup
DROP TABLE scoring1_bulk CASCADE;
DROP TABLE scoring1_incr CASCADE;
DROP EXTENSION pg_textsearch CASCADE;
